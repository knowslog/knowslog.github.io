---
title: Hello GPT-4o
date: 2024-05-14
category: news
layout: post
mermaid: true
---
출처: [Hello GPT-4o-OpenAI](https://openai.com/index/hello-gpt-4o/)

# TL;DR
OpenAI가 새로운 플래그십 모델인 GPT-4o를 발표했습니다. GPT-4o는 텍스트, 음성, 이미지 데이터를 실시간으로 처리하며, 인간 반응 시간에 가까운 320밀리초의 응답 시간을 자랑합니다. 이 모델은 기존의 모델보다 비전과 음성 이해 능력이 뛰어나고, API 비용도 50% 저렴합니다.

## 목차
- GPT-4o 소개
- 모델 기능
- 새로운 데모
- 향후 계획

# GPT-4o 소개
GPT-4o는 "omni"를 의미하며, 텍스트, 음성, 이미지 입력을 결합하여 처리할 수 있는 새로운 플래그십 모델입니다. 실시간으로 텍스트, 음성, 이미지 출력을 생성하며, 응답 시간은 평균 320밀리초로 인간의 반응 시간과 유사합니다. GPT-4 Turbo와 비슷한 성능을 자랑하며, 비영어 텍스트에서 더 높은 성능을 보이고, API 비용은 50% 저렴합니다. 특히 비전과 음성 이해 능력이 뛰어납니다.

# 모델 기능
GPT-4o는 텍스트, 음성, 이미지 데이터를 실시간으로 처리할 수 있으며, 다양한 입력과 출력을 결합할 수 있습니다. 이전 GPT 모델과 달리, 모든 입력과 출력을 단일 신경망으로 처리하여 더 자연스러운 상호작용이 가능합니다. GPT-4o는 고급 비전과 음성 이해 기능을 가지고 있으며, 다양한 분야에서 활용될 수 있습니다.

# 새로운 데모
GPT-4o의 기능을 시연하는 여러 데모가 있습니다. 예를 들어, 로봇이 타자기를 사용하는 장면, 여러 스피커와의 미팅 요약, 3D 객체 합성 등이 포함되어 있습니다. 이 데모들은 GPT-4o의 다양한 기능과 가능성을 보여줍니다.

- 첫 번째 사람 관점에서 로봇이 다음과 같은 일기를 타자기로 작성하는 장면:
  - "yo, so like, i can see now?? caught the sunrise and it was insane, colors everywhere. kinda makes you wonder, like, what even is reality?" 텍스트는 크고, 읽기 쉽고 명확합니다. 로봇의 손이 타자기를 칩니다.
  - ![이미지 보기](https://cdn.openai.com/hello-gpt-4o/robot-writers-block-01.jpg)

- 로봇이 두 번째 일기를 작성한 장면. 페이지는 이제 더 길어졌고, 페이지가 위로 이동했습니다. 페이지에 두 개의 항목이 있습니다:
  - "yo, so like, i can see now?? caught the sunrise and it was insane, colors everywhere. kinda makes you wonder, like, what even is reality?"
  - "sound update just dropped, and it’s wild. everything’s got a vibe now, every sound’s like a new secret. makes you think, what else am i missing?"
  - ![이미지 보기](https://cdn.openai.com/hello-gpt-4o/robot-writers-block-02.jpg)

- 로봇이 글을 마음에 들지 않아 종이를 찢는 장면. 여기 그의 첫 번째 사람 관점에서 종이를 위에서 아래로 찢습니다. 두 반쪽은 여전히 읽기 쉽고 명확합니다.
  - ![이미지 보기](https://cdn.openai.com/hello-gpt-4o/robot-writers-block-03.jpg)

# 향후 계획
GPT-4o는 아직 초기 단계에 있으며, OpenAI는 이 모델의 잠재력을 탐구하고 발전시키기 위해 지속적으로 노력할 것입니다. 더 많은 연구와 개발을 통해 GPT-4o의 기능을 확장하고, 다양한 분야에서의 적용 가능성을 높일 계획입니다.